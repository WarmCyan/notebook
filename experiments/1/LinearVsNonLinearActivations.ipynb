{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Linear vs Non-Linear Activations in Neural Networks\n",
    "\n",
    "(Initially started 7/8/2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Prior, I've had misconceptions about how activation functions affect how a neural network works. Largely, I felt that it would make sense to use \"linear neural networks\" for learning something like a math formula. (A more linear-regression type problem?) The only context where I had explicitly thought about activation functions in a non-linear neural network, I apparently assumed them to be perfectly sigmoidal, like many examples online! I have since discovered ReLU's are a thing, and at least vaguely understand the limitations of a linear neural network as compared to a non-linear neural network <span style='color: red'>[TODO: Prove this or find citation?]</span> but I'm continuing this experiment to see how different activations compare for different problem types.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do different activation functions like ReLU and sigmoidal compare on different types of problems such as regression versus classification? How do they compare to purely linear neural networks? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some different types of activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identity** - This is just a linear activation. A neural network that uses only identity activation functions is a linear neural network. Interestingly, regardless of how many layers a linear neural network has, it's functionally equivalent to a single layer neural network (making it a glorified affine transformation) [2] <span style='color: red'>[TODO: prove this?]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU** - Rectified Linear Unit. This is essentially just the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[1] [https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function)  \n",
    "[2] [http://people.whitman.edu/~hundledr/courses/M350S08/Ch10.pdf](http://people.whitman.edu/~hundledr/courses/M350S08/Ch10.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
