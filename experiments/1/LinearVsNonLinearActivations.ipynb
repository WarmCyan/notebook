{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Linear vs Non-Linear Activations in Neural Networks\n",
    "\n",
    "(Initially started 7/8/2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Prior, I've had misconceptions about how activation functions affect how a neural network works. Largely, I felt that it would make sense to use \"linear neural networks\" for learning something like a math formula. (A more linear-regression type problem?) The only context where I had explicitly thought about activation functions in a non-linear neural network, I apparently assumed them to always be sigmoidal, like many examples online! I have since discovered ReLU's are a thing, and at least vaguely understand the limitations of a linear neural network as compared to a non-linear neural network <span style='color: red'>[TODO: Prove this or find citation?]</span> but I'm continuing this experiment to see how different activations compare for different problem types.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do different activation functions like ReLU and logistic compare on different types of problems such as regression versus classification? How do they compare to purely linear neural networks? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nbhelper as nb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some different types of activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identity** - This is just a linear activation. A neural network that uses only identity activation functions is a linear neural network. Interestingly, regardless of how many layers a linear neural network has, it's functionally equivalent to a single layer neural network (making it a glorified affine transformation) [2]. <span style='color: red'>[TODO: prove this?]</span>\n",
    "\n",
    "It has a simple formula:\n",
    "$$\n",
    "f(x)=x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAACFCAYAAACAJLCMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXj5gagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f76715d5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.plot_identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU** - Rectified Linear Unit. This is essentially just the identity activation function, but it introduces non-linearity by zeroing anything less than $x=0$. This (to me) seems especially powerful in terms of still appropriately being able to handle regression problems, while still allowing for the greater complexity a non-linear multi-layer network can provide? ReLU's are one of the most popular activation networks in deep neural networks [3].\n",
    "\n",
    "Similarly simple formula: \n",
    "$$\n",
    "f(x)=\\textrm{max}(0,x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIgAAACFCAYAAACAJLCMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f763e538f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb.plot_relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'>[TODO: add logistic and tanh]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment will consist of a set of different problems, and for each problem, I'll run a network for several different architectures for several different activation function types:\n",
    "\n",
    "**Architectures:**\n",
    "<span style='color: red'>[TODO: TBD]</span>\n",
    "\n",
    "**Activation Functions:**\n",
    "- Identity (linear)\n",
    "- ReLU\n",
    "- Logistic\n",
    "- TanH\n",
    "\n",
    "**Problems:**\n",
    "- Generated addition data, learn the formula $f(i_1,i_2)=i_1+i_2$\n",
    "- Generated exponential data, learn the formula $f(i_1,i_2)={i_1}^{i_2}$\n",
    "- Generated addition data, but inputs represented in binary\n",
    "- Generated exponential data, but inputs represented in binary\n",
    "- Generated classification data for if input point is within circle: \n",
    "$\n",
    "f(i_1,i_2) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & : x^2 + y^2 \\leq 1 \\\\\n",
    "            0 & : x^2 + y^2 > 1\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$\n",
    "- Generated quadratic formula data: $f(a,b,c)=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$\n",
    "<span style='color: red'>[TODO: add imaginary detection in quadratic?]</span>\n",
    "- <span style='color: red'>[TODO: pick common classification dataset]</span>\n",
    "\n",
    "**Data Collection:**\n",
    "- Architecture\n",
    "- Activation function\n",
    "- Training data row count\n",
    "- <span style='color: orange'>Epoch count? Epoch size?</span>\n",
    "- Training time\n",
    "- (Regression) Mean squared error [4]\n",
    "- (Binary classification) Confusion matrix stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color: red'>NOTES</span>\n",
    "\n",
    "Read: [http://journal.frontiersin.org/article/10.3389/fncom.2014.00043/full](http://journal.frontiersin.org/article/10.3389/fncom.2014.00043/full) for more info on metrics etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[1] [https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function)  \n",
    "[2] [http://people.whitman.edu/~hundledr/courses/M350S08/Ch10.pdf](http://people.whitman.edu/~hundledr/courses/M350S08/Ch10.pdf)  \n",
    "[3] [https://en.wikipedia.org/wiki/Rectifier_(neural_networks)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))  \n",
    "[4] [https://en.wikipedia.org/wiki/Mean_squared_error](https://en.wikipedia.org/wiki/Mean_squared_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
